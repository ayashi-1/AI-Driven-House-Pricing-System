# -*- coding: utf-8 -*-
"""ny-housing-market-neural-network_modified.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cts7eb7v6-72IQ-xFd3UMbOZbypSyzcc
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

"""This dataset contains prices of New York houses, providing valuable insights into the real estate market in the region. It includes information such as broker titles, house types, prices, number of bedrooms and bathrooms, property square footage, addresses, state, administrative and local areas, street names, and geographical coordinates.

1. BROKERTITLE: Title of the broker
2. TYPE: Type of the house
3. PRICE: Price of the house
4. BEDS: Number of bedrooms
5. BATH: Number of bathrooms
6. PROPERTYSQFT: Square footage of the property
7. ADDRESS: Full address of the house
8. STATE: State of the house
9. MAIN_ADDRESS: Main address information
10. ADMINISTRATIVE_AREA_LEVEL_2: Administrative area level 2 information
11. LOCALITY: Locality information
12. SUBLOCALITY: Sublocality information
13. STREET_NAME: Street name
14. LONG_NAME: Long name
15. FORMATTED_ADDRESS: Formatted address
16. LATITUDE: Latitude coordinate of the house
17. LONGITUDE: Longitude coordinate of the house

# **Data** **Loading**
"""

import pandas as pd
from google.colab import files
import io

uploaded = files.upload()

df = pd.read_csv("/content/NY-House-Dataset.csv")
df



"""
# **Data Cleaning and Preprocessing**"""

df.head()

df.shape

df.columns

df.info()

df.describe()

"""Checking null values"""

df.isnull().sum()

"""Checking duplicates"""

df.duplicated().sum()

df.drop_duplicates(inplace=True)

df.duplicated().sum()

df = df.drop('BROKERTITLE', axis=1)
df = df.drop('MAIN_ADDRESS', axis=1)
df = df.drop('FORMATTED_ADDRESS', axis=1)

"""# **Feature Engineering**"""

df_encoded = pd.get_dummies(data=df, columns=['TYPE', 'STATE', 'ADMINISTRATIVE_AREA_LEVEL_2', 'LOCALITY',
       'SUBLOCALITY', 'STREET_NAME'])
print(df_encoded)

"""Data visualization

Plot the bar plot for the TYPE column to show the categorical values the distribution for each category
"""

import matplotlib.pyplot as plt
import seaborn as sns

counter=df['TYPE'].value_counts()
plt.bar(counter.index,counter,color='green')
plt.xticks(rotation=90)

# Add labels and title
plt.xlabel('type of the house')
plt.ylabel('count')
plt.title('Count of the different types of the house')

# Show the plot
plt.show()

plt.bar(counter.index[:20],counter[:20],color='green')
plt.xticks(rotation=90)

# Add labels and title
plt.xlabel('street name')
plt.ylabel('count')
plt.title('Count of the houses on the different street names')

# Show the plot
plt.show()

counter=df['STREET_NAME'].value_counts()
counter

"""Calculate skewness for each column"""

for column in df.select_dtypes(include=[np.number]).columns:
    skewness_per_column = df[column].skew()
    print(f"Skewness for {column}: {skewness_per_column}")

"""The skewness for the numeric columns shows the extent of the deviation of the graph for those columns.  Outliers should be deleted."""

plt.boxplot(df['BEDS'])

# Add labels and title
plt.xlabel('num of beds')
plt.ylabel('Frequency')
plt.title('beds')

# Show the plot
plt.show()

"""The values above 10 are outliers."""

sns.histplot(df['BEDS'], bins=20, color='green', edgecolor='white',kde=True)

# Add labels and title
plt.xlabel('num of beds')
plt.ylabel('Frequency')
plt.title('num od beds dist')

# Show the plot
plt.show()

"""The distribution is right skewness."""

plt.boxplot(df['BATH'])

# Add labels and title
plt.xlabel('num of baths')
plt.ylabel('Frequency')
plt.title('baths')

# Show the plot
plt.show()

"""The values above 10 baths are outliers."""

sns.histplot(df['BATH'], bins=20, color='green', edgecolor='white',kde=True)

# Add labels and title
plt.xlabel('num of BATH')
plt.ylabel('Frequency')
plt.title('num of BATH dist')

# Show the plot
plt.show()

"""The bath column is right skewness."""

plt.boxplot(df['PROPERTYSQFT'])

# Add labels and title
plt.xlabel('num of PROPERTYSQFT')
plt.ylabel('Frequency')
plt.title('PROPERTYSQFT')

# Show the plot
plt.show()

sns.histplot(df['PROPERTYSQFT'], bins=20, color='green', edgecolor='white',kde=True)

# Add labels and title
plt.xlabel('num of PROPERTYSQFT')
plt.ylabel('Frequency')
plt.title('num of PROPERTYSQFT dist')

# Show the plot
plt.show()

"""The PROPERTYSQFT column is right skewness."""

plt.boxplot(df['PRICE'])

# Add labels and title
plt.xlabel('num of PRICE')
plt.ylabel('Frequency')
plt.title('PRICE')

# Show the plot
plt.show()

sns.histplot(df['PRICE'], bins=20, color='green', edgecolor='white',kde=True)

# Add labels and title
plt.xlabel('num of PRICE')
plt.ylabel('Frequency')
plt.title('num of PRICE dist')

# Show the plot
plt.show()

"""The PRICE column is right skewness."""

plt.boxplot(df['LATITUDE'])

# Add labels and title
plt.xlabel('num of LATITUDE')
plt.ylabel('Frequency')
plt.title('LATITUDE')

# Show the plot
plt.show()

""" Outliers are not present."""

sns.histplot(df['LATITUDE'], bins=20, color='green', edgecolor='white',kde=True)

# Add labels and title
plt.xlabel('num of LATITUDE')
plt.ylabel('Frequency')
plt.title('num od LATITUDE dist')

# Show the plot
plt.show()

"""The LATITUDE column is a little right skewness."""

plt.boxplot(df['LONGITUDE'])

# Add labels and title
plt.xlabel('num of LONGITUDE')
plt.ylabel('Frequency')
plt.title('LONGITUDE')

# Show the plot
plt.show()

"""The values under -74.05 LONGITUDE are outliers."""

sns.histplot(df['LONGITUDE'], bins=20, color='green', edgecolor='white',kde=True)

# Add labels and title
plt.xlabel('num of LONGITUDE')
plt.ylabel('Frequency')
plt.title('num od LONGITUDE dist')

# Show the plot
plt.show()

"""The IQR for each column."""

filter_df=df.iloc[:,[1,2,3,4,12,13]]
filter_df

for column in filter_df.columns:
    Q1 = filter_df[column].quantile(0.25)
    Q3 = filter_df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df[column] = filter_df[(filter_df[column] >= lower_bound) & (filter_df[column] <= upper_bound)][column]

df.info()

df.isna().sum()

df.dropna(inplace=True)

df.isna().sum()

"""Calculate skewness for each column after compute the IQR for each column:"""

for column in df.select_dtypes(include=[np.number]).columns:
    skewness_per_column = df[column].skew()
    print(f"Skewness for {column}: {skewness_per_column}")

"""The values of skewness are now smaller but skewness for PRICE is still too high.."""

df['PRICE'] = np.log(df['PRICE']+1)

for column in df.select_dtypes(include=[np.number]).columns:
    skewness_per_column = df[column].skew()
    print(f"Skewness for {column}: {skewness_per_column}")

df.describe()

df.info()

# Dropping usless columns.
df.drop(columns=['ADDRESS','LONG_NAME'],inplace=True)
df.head()

"""Checking which columns are string and which are numerical."""

string_columns = df.select_dtypes(include="object").columns
print("String columns are = ", string_columns)
numerical_columns = df.select_dtypes(include=["int64", "float64"]).columns
print("Numerical columns are = ", numerical_columns)

df[numerical_columns].corr()

sns.set(style="whitegrid", font_scale=1)

plt.figure(figsize=(10,10))
plt.title('Pearson Correlation Matrix',fontsize=25)
sns.heatmap(df[numerical_columns].corr(),linewidths=0.25,vmax=0.7,square=True,cmap="GnBu",linecolor='w',
            annot=True, annot_kws={"size":7}, cbar_kws={"shrink": .7})

"""Encoding string columns."""

df['TYPE'].value_counts()

print(pd.get_dummies(df['TYPE']))

df['STATE'].value_counts()

print(pd.get_dummies(df['STATE']))

df['ADMINISTRATIVE_AREA_LEVEL_2'].value_counts()

print(pd.get_dummies(df['ADMINISTRATIVE_AREA_LEVEL_2']))

df['LOCALITY'].value_counts()

print(pd.get_dummies(df['LOCALITY']))

df['SUBLOCALITY'].value_counts()

print(pd.get_dummies(df['SUBLOCALITY']))

df['STREET_NAME'].value_counts()

print(pd.get_dummies(df['STREET_NAME']))

df_encoded = pd.get_dummies(data=df, columns=['TYPE', 'STATE', 'ADMINISTRATIVE_AREA_LEVEL_2', 'LOCALITY',
       'SUBLOCALITY', 'STREET_NAME'])
print(df_encoded)

df_encoded.describe()

"""# **Model Training**"""

X = df_encoded.drop('PRICE', axis=1)

y = df_encoded['PRICE']

"""Splitting the data"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=20)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

"""Normalizing / scalling the data

We scale the feature data. To prevent data leakage from the test set, we only fit our scaler to the training set.
"""

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()

"""Fit and transform"""

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""Everything has been scaled between 1 and 0"""

print('Max:', X_train.max())
print('Min:', X_train.min())

"""Creating a model

We estimate the number of neurons (units) from our features. The optimizer is asking how you want to perform this gradient descent. In this case we are using the Adam optimizer and the mean square error loss function.
"""



# Commented out IPython magic to ensure Python compatibility.
# %pip install tensorflow

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential()
# Input layer
model.add(Dense(9, activation='relu'))

# Hidden layers
model.add(Dense(9, activation='relu'))
model.add(Dense(9, activation='relu'))
model.add(Dense(9, activation='relu'))

# Output layer
model.add(Dense(1))

model.compile(optimizer='Adam', loss='mse')

"""Training the model

Now we can fit the model into the data. Since, the dataset is large, we are going to use batches of the power of 2 (32, 64, 128, 256...). In this case we are using 64. The smaller batch size, the longer is going to take.
"""

model.fit(x=X_train, y=y_train.values,
          validation_data=(X_test, y_test.values),
          batch_size=64, epochs=150)

"""Trainning loss per epoch"""

losses = pd.DataFrame(model.history.history)

plt.figure(figsize=(15,5))
sns.lineplot(data=losses, lw=3)
plt.xlabel('Epochs')
plt.ylabel('')
plt.title('Training Loss per Epoch')
sns.despine()

"""Evaluation on test data
Regression Evaluation Metrics:
- Mean Absolute Error MAE is the mean of the absolute values of the errors
- Mean Squared Error (MSE) is the mean of the squared errors
- Root Mean Squared Error (RMSE) is the square root of the mean of the squared errors
"""

from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score
from sklearn.metrics import classification_report,confusion_matrix

predictions = model.predict(X_test)

print('MAE: ', mean_absolute_error(y_test, predictions))
print('MSE: ', mean_squared_error(y_test, predictions))
print('RMSE: ', np.sqrt(mean_squared_error(y_test, predictions)))
print('Variance Regression Score: ', explained_variance_score(y_test, predictions))

print(df_encoded['PRICE'].describe())

"""Model predictions vs. perfect fit

We can compare the model predictions with a perfect fit to see how accurate the model is.
The red line represents the perfect prediction.
"""

f, axes = plt.subplots(1, 2,figsize=(15,5))

# Our model predictions
plt.scatter(y_test,predictions)

# Perfect predictions
plt.plot(y_test,y_test,'r')

errors = y_test.values.reshape(1105, 1) - predictions
sns.distplot(errors, ax=axes[0])

sns.despine(left=True, bottom=True)
axes[0].set(xlabel='Error', ylabel='', title='Error Histogram')
axes[1].set(xlabel='Test True Y', ylabel='Model Predictions', title='Model Predictions vs Perfect Fit')

"""Predicting on a brand new house

We are going to use the model to predict the price on a brand-new house. We are going to choose the first house of the data set and drop the price. single_house is going to have all the features that we need to predict the price. After that we need to reshape the variable and scale the features.
"""

# features of new house
single_house = df_encoded.drop('PRICE',axis=1).iloc[0]
print(f'Features of new house:\n{single_house}')

# reshape the numpy array and scale the features
single_house = scaler.transform(single_house.values.reshape(-1, 535))

# run the model and get the price prediction
print('\nPrediction Price:',model.predict(single_house)[0,0])

# original price
print('\nOriginal Price:',df.iloc[0]['PRICE'])

from tensorflow.keras.layers import Dropout
from sklearn.metrics import r2_score

# Use already scaled data from previous cells
X_train_scaled = X_train
X_test_scaled = X_test

# Define the model
model = Sequential()
model.add(Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(32, activation='relu'))
model.add(Dense(1))  # Regression output

# Compile the model
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train the model
history = model.fit(X_train_scaled, y_train, validation_split=0.1,
                    epochs=100, batch_size=32, verbose=1)

# Predict and Evaluate
y_pred_nn = model.predict(X_test_scaled)

mse_nn = mean_squared_error(y_test, y_pred_nn)
rmse_nn = np.sqrt(mse_nn)
mae_nn = mean_absolute_error(y_test, y_pred_nn)
r2_nn = r2_score(y_test, y_pred_nn)

# Results
print(" Neural Network Performance:")
print(f" MSE  : {mse_nn:.2f}")
print(f" RMSE : {rmse_nn:.2f}")
print(f" MAE  : {mae_nn:.2f}")
print(f" R²   : {r2_nn:.4f}")

# Commented out IPython magic to ensure Python compatibility.
# %pip install xgboost

import xgboost as xgb
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

# 1. Define the model
xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)

# 2. Train the model
xgb_model.fit(X_train, y_train)

# 3. Predict
y_pred_xgb = xgb_model.predict(X_test)

# 4. Evaluate
mse_xgb = mean_squared_error(y_test, y_pred_xgb)
rmse_xgb = np.sqrt(mse_xgb)
mae_xgb = mean_absolute_error(y_test, y_pred_xgb)
r2_xgb = r2_score(y_test, y_pred_xgb)

# 5. Print results
print(" XGBoost Performance:")
print(f"   MSE  : {mse_xgb:.2f}")
print(f"   RMSE : {rmse_xgb:.2f}")
print(f"   MAE  : {mae_xgb:.2f}")
print(f"   R^2  : {r2_xgb:.4f}")

# Commented out IPython magic to ensure Python compatibility.
# %pip install lightgbm

import lightgbm as lgb
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

lgb_model = lgb.LGBMRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
lgb_model.fit(X_train, y_train)
y_pred_lgb = lgb_model.predict(X_test)

mse = mean_squared_error(y_test, y_pred_lgb)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred_lgb)
r2 = r2_score(y_test, y_pred_lgb)

print(" LightGBM Performance:")
print(f"   MSE  : {mse:.2f}")
print(f"   RMSE : {rmse:.2f}")
print(f"   MAE  : {mae:.2f}")
print(f"   R^2  : {r2:.4f}")

"""### Simple Ensemble Method (Averaging Predictions)"""

# Ensure you have predictions from the desired models (nn, xgb, lgb)
# If not, run the prediction cells first

# Average the predictions of the models
# Note: You might want to select a subset of your best-performing models
# or use weighted averaging based on individual model performance

# Check if all prediction variables exist before averaging
if 'y_pred_nn' in locals() and 'y_pred_xgb' in locals() and 'y_pred_lgb' in locals():
    ensemble_predictions = (y_pred_nn.flatten() + y_pred_xgb + y_pred_lgb) / 3

    # Evaluate the ensemble model
    mse_ensemble = mean_squared_error(y_test, ensemble_predictions)
    rmse_ensemble = np.sqrt(mse_ensemble)
    mae_ensemble = mean_absolute_error(y_test, ensemble_predictions)
    r2_ensemble = r2_score(y_test, ensemble_predictions)

    print(" Ensemble Model Performance (Averaging):")
    print(f"   MSE  : {mse_ensemble:.2f}")
    print(f"   RMSE : {rmse_ensemble:.2f}")
    print(f"   MAE  : {mae_ensemble:.2f}")
    print(f"   R^2  : {r2_ensemble:.4f}")
else:
    print("Please ensure predictions from the Neural Network, XGBoost, and LightGBM models are available before creating the ensemble.")

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8,6))
plt.scatter(y_test, ensemble_predictions, alpha=0.5, color='purple')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # perfect prediction line
plt.xlabel('Actual Price (log transformed)')
plt.ylabel('Ensemble Predicted Price (log transformed)')
plt.title('Ensemble Model: Actual vs Predicted Prices (Log Transformed)')
plt.grid(True)
plt.show()

"""# **Model Evaluation and Comparison**"""

# Show first 10 predictions vs actual values
for i in range(10):
    print(f"Predicted: ₹{y_pred_xgb[i]:,.2f}  |  Actual: ₹{y_test.values[i]:,.2f}")

import matplotlib.pyplot as plt

plt.figure(figsize=(8,6))
plt.scatter(y_test, y_pred_xgb, alpha=0.5, color='teal')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # perfect prediction line
plt.xlabel('Actual Price')
plt.ylabel('Predicted Price')
plt.title('XGBoost: Actual vs Predicted Prices')
plt.grid(True)
plt.show()

"""# **Prediction on New Data**"""

import numpy as np

# 1) Choose a test‐sample index (e.g. 0)
idx = 0

# 2) Grab its features (assumes X_test is your scaled test‐set array)
single_feat = X_test[idx].reshape(1, -1)

# 3) Predict (all outputs are log(price+1))
pred_log_nn  = model.predict(single_feat)[0][0]
pred_log_xgb = xgb_model.predict(single_feat)[0]
pred_log_lgb = lgb_model.predict(single_feat)[0]

# 4) True log-price
actual_log = y_test.iloc[idx]

# 5) Back-transform to dollars
actual_price    = np.expm1(actual_log)
pred_price_nn   = np.expm1(pred_log_nn)
pred_price_xgb  = np.expm1(pred_log_xgb)
pred_price_lgb  = np.expm1(pred_log_lgb)

# 6) Print comparison
print(f"Actual Price:          ${actual_price:,.2f}")
print(f"Neural Net Predicted:  ${pred_price_nn:,.2f}")
print(f"XGBoost Predicted:     ${pred_price_xgb:,.2f}")
print(f"LightGBM Predicted:    ${pred_price_lgb:,.2f}")

"""### Cross-Validation Example (using XGBoost)"""

from sklearn.model_selection import cross_val_score

# Using the XGBoost model as an example
scores = cross_val_score(xgb_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')
rmse_scores = np.sqrt(-scores)

print(f"Cross-validation RMSE scores: {rmse_scores}")
print(f"Mean Cross-validation RMSE: {rmse_scores.mean():.2f}")
print(f"Standard Deviation of Cross-validation RMSE: {rmse_scores.std():.2f}")